# CodeQL Configuration

# Path to CodeQL executable (required)
# Examples:
#   Linux/macOS: /usr/local/bin/codeql or /path/to/codeql
#   Windows: IMPORTANT - Path MUST end with .cmd
#            Example: C:\path\to\codeql\codeql.cmd
#            Use forward slashes or escaped backslashes: C:/path/to/codeql/codeql.cmd
#            Or use raw string format: r"C:\path\to\codeql\codeql.cmd"
CODEQL_PATH="your_codeql_path"

# GitHub Configuration (optional, for higher rate limits)
# Get token from: https://github.com/settings/tokens
# GITHUB_TOKEN=ghp_your_token_here

# GitHub Enterprise Server (optional)
# For GitHub Enterprise, set the API URL. Default: https://api.github.com
# GITHUB_API_URL=https://github.your-company.com/api/v3
#
# SSL Certificate Verification (optional)
# Set to false for GitHub Enterprise with self-signed or internal CA certificates
# GITHUB_SSL_VERIFY=false

# LLM Configuration
# Copy this file to .env and fill in your API keys

# Provider selection (required)
# Allowed providers: openai, azure, gemini, bedrock, anthropic, mistral, groq, ollama

# Model name (required, provider-specific)
# Examples by provider:
#   OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
#   Azure: gpt-4o, gpt-4
#   Google AI Studio: gemini-2.5-flash, gemini-2.0-flash
#   Bedrock: anthropic.claude-3-5-sonnet-20241022-v2:0, anthropic.claude-3-haiku-20240307-v1:0

# Optional: Override default LLM parameters
# NOTE:
# Do NOT increase these values unless you fully understand the impact.
# Lower values keep the model stable and deterministic â€” critical for security analysis.
# Higher values may cause the model to become inconsistent, creative, or hallucinate results.
# Recommended: leave these at their default values.
# LLM_TEMPERATURE=0.2
# LLM_TOP_P=0.2

# ============================================================================
# Provider-Specific Configuration
# ============================================================================
# Uncomment and fill in the section for your chosen provider

# ----------------------------------------------------------------------------
# OpenAI
# ----------------------------------------------------------------------------
PROVIDER=openai
MODEL=gpt-4o
OPENAI_API_KEY="your_api_key"

# ----------------------------------------------------------------------------
# Azure OpenAI
# ----------------------------------------------------------------------------
# AZURE_OPENAI_API_KEY="your_api_key"
# AZURE_OPENAI_ENDPOINT="https://your-name.openai.azure.com/"
# AZURE_OPENAI_API_VERSION="2024-08-01-preview"
# PROVIDER=azure
# MODEL=gpt-4o

# ----------------------------------------------------------------------------
# Google AI Studio
# ----------------------------------------------------------------------------
# GOOGLE_API_KEY="your_api_key"
# PROVIDER=gemini
# MODEL=gemini-2.5-flash

# ----------------------------------------------------------------------------
# AWS Bedrock
# ----------------------------------------------------------------------------
# PROVIDER=bedrock
# MODEL=us.anthropic.claude-3-5-sonnet-20241022-v2:0
# AWS_REGION_NAME=us-east-1
#
# Authentication (choose one method):
#
# Method 1: AWS SSO (recommended for development)
# First run: aws sso login --profile your-profile
# AWS_PROFILE=your-profile
#
# Method 2: Static credentials (IAM user)
# AWS_ACCESS_KEY_ID=AKIA...
# AWS_SECRET_ACCESS_KEY=...
#
# Method 3: Temporary credentials (STS)
# AWS_ACCESS_KEY_ID=ASIA...
# AWS_SECRET_ACCESS_KEY=...
# AWS_SESSION_TOKEN=...
#
# Note: Tool calling requires Claude 3.x, Mistral, or Cohere Command R models.
# Amazon Titan and Meta Llama models do not support function calling.

# Logging Configuration

# DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Optional: path to log file (e.g., logs/vulnhalla.log)
# If empty or commented out, no file logging is used
# LOG_FILE=logs/vulnhalla.log
LOG_FILE=

# default or json
LOG_FORMAT=default

# Console format control:
# - Default: INFO messages are minimal (message only) 
#           WARNING/ERROR/CRITICAL use simple format (LEVEL - message)
# - If LOG_VERBOSE_CONSOLE=true: 
#   WARNING/ERROR/CRITICAL use full format
#   (timestamp - logger - level - message)
# - INFO always remains minimal regardless of verbose mode
LOG_VERBOSE_CONSOLE=false

# Control third-party library logging verbosity (LiteLLM, urllib3, requests). Default: ERROR
THIRD_PARTY_LOG_LEVEL=ERROR